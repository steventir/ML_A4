{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "ENV_NAME = \"CartPole-v0\"\n",
    "\n",
    "# FEAR MODEL\n",
    "fear_on = True\n",
    "fear_radius = 5\n",
    "fear_factor = 0.5\n",
    "fear_linear = 10000 # Number of steps for the adjusted fear_factor reach the value of fear factor (same as epsilon)\n",
    "fear_warmup = 1000\n",
    "FEAR_LEARNING_RATE = 0.0001\n",
    "\n",
    "# DQN\n",
    "GAMMA = 0.9 # Discount factor\n",
    "INITIAL_EPSILON = 0.5 # Initial value for epsilon-greedy\n",
    "FINAL_EPSILON = 0.01 # Final value for epsilon-greedy\n",
    "EXPLORATION_STEPS = 10000 # Number of steps till epsilon reaches its final value\n",
    "MEMORY_REPLAY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER_LEARNING_RATE = 0.0001\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        \"\"\" Initilizes the agent: replay memory, neural network \"\"\"\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.num_observations = env.observation_space.shape[0]\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "        self.time_step = 0\n",
    "        \n",
    "        # Create memory buffer for replay\n",
    "        self.memory_replay = deque()\n",
    "        \n",
    "        # Create Q network and define loss\n",
    "        self.state_input, self.q_values = self.build_network()\n",
    "        self.action_input, self.y_input, self.loss, self.optimizer = self.build_training_ops() \n",
    "        \n",
    "        # Create Fear network and define loss\n",
    "        self.avg_fear = 0\n",
    "        self.danger_states = []\n",
    "        self.safe_states = []\n",
    "        \n",
    "        self.fear_state_input, self.fear_scores = self.build_fear_network()\n",
    "        self.fear_y_input, self.fear_loss, self.fear_optimizer = self.build_fear_training_ops()\n",
    "        \n",
    "        # Create session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        \n",
    "        # Create summaries\n",
    "        self.total_reward = 0 \n",
    "        self.test_total_reward = 0\n",
    "        self.total_loss = 0\n",
    "        self.total_fear_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_network(self):\n",
    "        \"\"\" Constructs the neural network with 1 hidden layer to approximate the Q function \"\"\"\n",
    "        W1 = tf.Variable(tf.truncated_normal([self.num_observations,16]))\n",
    "        b1 = tf.Variable(tf.constant(0.01, shape=[16]))\n",
    "        W2 = tf.Variable(tf.truncated_normal([16,self.num_actions]))\n",
    "        b2 = tf.Variable(tf.constant(0.01, shape=[self.num_actions]))\n",
    "\n",
    "        state_input = tf.placeholder(dtype=tf.float32)\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(state_input,W1) + b1)\n",
    "        q_values = tf.matmul(hidden_layer, W2) + b2\n",
    "\n",
    "        return state_input, q_values\n",
    "        \n",
    "    def build_training_ops(self):\n",
    "        \"\"\" Defines the loss function \"\"\"\n",
    "        y_input = tf.placeholder(dtype=tf.float32)\n",
    "        action_input = tf.placeholder(dtype=tf.int32)\n",
    "        action_one_hot = tf.one_hot(action_input, self.num_actions, on_value=1.0, off_value=0.0)\n",
    "\n",
    "        q_action = tf.reduce_sum(tf.multiply(self.q_values, action_one_hot), axis=1)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.square(y_input - q_action))\n",
    "        optimizer = tf.train.AdamOptimizer(OPTIMIZER_LEARNING_RATE).minimize(loss)\n",
    "        \n",
    "        return action_input, y_input, loss, optimizer\n",
    "    \n",
    "    def build_fear_network(self):\n",
    "        W1 = tf.Variable(tf.truncated_normal([self.num_observations,16]))\n",
    "        b1 = tf.Variable(tf.constant(0.01, shape=[16]))\n",
    "        W2 = tf.Variable(tf.truncated_normal([16,1]))\n",
    "        b2 = tf.Variable(tf.constant(0.01, shape=[1]))\n",
    "\n",
    "        fear_state_input = tf.placeholder(dtype=tf.float32)\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(fear_state_input,W1) + b1)\n",
    "        fear_scores = tf.matmul(hidden_layer, W2) + b2\n",
    "\n",
    "        return fear_state_input, fear_scores\n",
    "        \n",
    "    def build_fear_training_ops(self):\n",
    "        fear_y_input = tf.placeholder(dtype=tf.float32)\n",
    "        \n",
    "        yhat = tf.minimum(tf.maximum(tf.reduce_sum(self.fear_scores, axis=1), .01), 0.99)\n",
    "        \n",
    "        fear_loss = tf.reduce_mean( - (fear_y_input * tf.log(yhat) + (1-fear_y_input) * tf.log(1-yhat)))\n",
    "        fear_optimizer = tf.train.AdamOptimizer(FEAR_LEARNING_RATE).minimize(fear_loss)\n",
    "        \n",
    "        return fear_y_input, fear_loss, fear_optimizer\n",
    "        \n",
    "        \n",
    "    def run(self, state, action, reward, next_state, terminal):  \n",
    "        \"\"\" Updates knowledge \"\"\"\n",
    "        # Store transition in memory\n",
    "        self.memory_replay.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        if len(self.memory_replay) > MEMORY_REPLAY_SIZE:\n",
    "            self.memory_replay.popleft()\n",
    "            \n",
    "        if len(self.memory_replay) > BATCH_SIZE:\n",
    "            self.train_network()\n",
    "            \n",
    "        self.total_reward += reward\n",
    "        self.duration += 1.0\n",
    "        \n",
    "        if terminal:\n",
    "            stats = [self.total_reward, self.total_loss / self.duration, self.test_total_reward, self.total_fear_loss / self.duration]\n",
    "            \n",
    "            for i in range(len(stats)):\n",
    "                self.sess.run(self.update_ops[i], feed_dict={\n",
    "                    self.summary_placeholders[i]: float(stats[i])\n",
    "                })\n",
    "            summary_str = self.sess.run(self.summary_op)\n",
    "            self.summary_writer.add_summary(summary_str, self.episode + 1)\n",
    "            \n",
    "            self.total_reward = 0\n",
    "            self.total_loss = 0\n",
    "            self.total_fear_loss = 0\n",
    "            self.duration = 0\n",
    "            self.episode += 1\n",
    "        \n",
    "    def explore(self, state):\n",
    "        \"\"\" Explores possible actions using epsilon-greedy with epsilon linearly decreasing \"\"\"\n",
    "        \n",
    "        if fear_on and self.time_step > fear_warmup: \n",
    "            fear_score = self.fear_scores.eval(feed_dict={self.fear_state_input:[state]})\n",
    "            self.avg_fear = self.avg_fear * .99 + fear_score * .01\n",
    "        \n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0, self.num_actions-1)\n",
    "        else:\n",
    "            return np.argmax(self.q_values.eval(feed_dict={self.state_input:[state]}))\n",
    "        \n",
    "        self.epsilon -= self.epsilon_step\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\" Get the best action : max Q \"\"\"\n",
    "        return np.argmax(self.q_values.eval(feed_dict={self.state_input:[state]}))\n",
    "        \n",
    "    def train_network(self):\n",
    "        \"\"\" Trains the network using mini batch \"\"\"\n",
    "        mini_batch = pd.DataFrame(random.sample(self.memory_replay, BATCH_SIZE), columns = ['state','action','reward','next_state','terminal'])\n",
    "        \n",
    "        q_values_batch = self.q_values.eval(feed_dict={self.state_input:mini_batch['next_state'].tolist()})\n",
    "        terminal_batch = mini_batch['terminal'] + 0 # convert True to 1, False to 0\n",
    "        \n",
    "        if fear_on and self.time_step > fear_warmup:\n",
    "            adjusted_fear_factor = np.min([fear_factor * float(self.time_step)/fear_linear, fear_factor])\n",
    "            fear_penalty = (1 - terminal_batch) * adjusted_fear_factor * np.max(self.fear_scores.eval(feed_dict={self.fear_state_input:mini_batch['next_state'].tolist()}), axis=1) + terminal_batch * 10 * adjusted_fear_factor\n",
    "            y_batch = mini_batch['reward'] + (1 - terminal_batch) * GAMMA * np.max(q_values_batch, axis=1) - fear_penalty\n",
    "        else:\n",
    "            y_batch = mini_batch['reward'] + (1 - terminal_batch) * GAMMA * np.max(q_values_batch, axis=1)\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss,self.optimizer], feed_dict={\n",
    "            self.y_input : y_batch.tolist(),\n",
    "            self.action_input : mini_batch['action'].tolist(),\n",
    "            self.state_input : mini_batch['state'].tolist()\n",
    "        })\n",
    "        \n",
    "        self.total_loss += loss\n",
    "        \n",
    "        if fear_on and self.time_step > fear_warmup:\n",
    "            if (len(self.danger_states) >= int(BATCH_SIZE/2)) and (len(self.safe_states) > int(BATCH_SIZE/2)):\n",
    "                fear_minibatch = random.sample(self.danger_states, int(BATCH_SIZE)/2) + random.sample(self.safe_states, int(BATCH_SIZE)/2)\n",
    "                fear_y_batch = [1] * int(BATCH_SIZE/2) + [0] * int(BATCH_SIZE/2)\n",
    "                fear_loss, _ = self.sess.run([self.fear_loss, self.fear_optimizer], feed_dict={\n",
    "                    self.fear_y_input: fear_y_batch,\n",
    "                    self.fear_state_input: fear_minibatch\n",
    "                })\n",
    "                self.total_fear_loss += fear_loss\n",
    "        self.time_step += 1\n",
    "        \n",
    "    def log_danger(self, new_danger_states):\n",
    "        self.danger_states += new_danger_states\n",
    "        \n",
    "    def log_safe(self, new_safe_states):\n",
    "        self.safe_states += new_safe_states\n",
    "        \n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + \"/Total_Reward/Episode\", episode_total_reward)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + \"/Avg_Loss/Episode\", episode_avg_loss)\n",
    "        episode_test_total_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + \"/Test_Total_Reward/Episode\", episode_test_total_reward)\n",
    "        episode_avg_fear_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + \"/Avg_Fear_Loss/Episode\", episode_avg_fear_loss)\n",
    "        \n",
    "        summary_vars = [episode_total_reward, episode_avg_loss, episode_test_total_reward, episode_avg_fear_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        \n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "    def record_test_reward(self, reward):\n",
    "        self.test_total_reward = reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode : ', 0, 'avg_reward:', 10.6)\n",
      "('episode : ', 100, 'avg_reward:', 9.1)\n",
      "('episode : ', 200, 'avg_reward:', 9.1)\n",
      "('episode : ', 300, 'avg_reward:', 14.3)\n",
      "('episode : ', 400, 'avg_reward:', 11.4)\n",
      "('episode : ', 500, 'avg_reward:', 15.0)\n",
      "('episode : ', 600, 'avg_reward:', 16.8)\n",
      "('episode : ', 700, 'avg_reward:', 21.5)\n",
      "('episode : ', 800, 'avg_reward:', 22.3)\n",
      "('episode : ', 900, 'avg_reward:', 36.1)\n",
      "('episode : ', 1000, 'avg_reward:', 41.1)\n",
      "('episode : ', 1100, 'avg_reward:', 67.2)\n",
      "('episode : ', 1200, 'avg_reward:', 157.9)\n",
      "('episode : ', 1300, 'avg_reward:', 200.0)\n",
      "('episode : ', 1400, 'avg_reward:', 200.0)\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 10000\n",
    "env = gym.make(ENV_NAME)\n",
    "agent = Agent(env)\n",
    "\n",
    "reached = False\n",
    "for episode in xrange(EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_observations = []\n",
    "    while not done:\n",
    "        action = agent.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.run(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_observations.append(state)\n",
    "        \n",
    "    if fear_on:\n",
    "        agent.log_danger(episode_observations[-fear_radius:])\n",
    "        agent.log_safe(episode_observations[:-fear_radius])\n",
    "        \n",
    "        for e in episode_observations:\n",
    "            del e\n",
    "        del episode_observations\n",
    "        episode_observations = []\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        total_reward = 0\n",
    "        for i in xrange(10): #10 test\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "#                 env.render()\n",
    "                action = agent.get_action(state)\n",
    "                state,reward,done,_ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "        avg_reward = total_reward / 10\n",
    "        agent.record_test_reward(avg_reward)\n",
    "        print('episode : ', episode, \"avg_reward:\", avg_reward)\n",
    "        if avg_reward >= 200:\n",
    "            if reached:\n",
    "                break\n",
    "            reached = True\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
