{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "ENV_NAME = \"CartPole-v0\"\n",
    "\n",
    "# FEAR MODEL\n",
    "fear_on = False\n",
    "fear_radius = 5\n",
    "fear_factor = 0.5\n",
    "fear_linear = 1000000\n",
    "fear_warmup = 20000\n",
    "\n",
    "# DQN\n",
    "GAMMA = 0.9 # Discount factor\n",
    "INITIAL_EPSILON = 0.5 # Initial value for epsilon-greedy\n",
    "FINAL_EPSILON = 0.01 # Final value for epsilon-greedy\n",
    "EXPLORATION_STEPS = 10000 # Number of steps till epsilon reaches its final value\n",
    "MEMORY_REPLAY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER_LEARNING_RATE = 0.0001\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.num_observations = env.observation_space.shape[0]\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "        self.time_step = 0\n",
    "        \n",
    "        # Create memory buffer for replay\n",
    "        self.memory_replay = deque()\n",
    "        \n",
    "        # Create Q network and define loss\n",
    "        self.state_input, self.q_values = self.build_network()\n",
    "        self.action_input, self.y_input, self.loss, self.optimizer = self.build_training_ops()\n",
    "        \n",
    "        # Create session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_network(self):\n",
    "        W1 = tf.Variable(tf.truncated_normal([self.num_observations,20]))\n",
    "        b1 = tf.Variable(0.01, dtype=tf.float32)\n",
    "        W2 = tf.Variable(tf.truncated_normal([20,self.num_actions]))\n",
    "        b2 = tf.Variable(0.01, dtype=tf.float32)\n",
    "        \n",
    "        state_input = tf.placeholder(dtype=tf.float32)\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(state_input,W1) + b1)\n",
    "        q_values = tf.matmul(hidden_layer, W2) + b2\n",
    "        \n",
    "        return state_input, q_values\n",
    "        \n",
    "    def build_training_ops(self): #action_input, y_input\n",
    "        action_input = tf.placeholder(dtype=tf.int32)\n",
    "        y_input = tf.placeholder(dtype=tf.float32)\n",
    "        \n",
    "        action_one_hot = tf.one_hot(action_input, self.num_actions, on_value=1.0, off_value=0.0)\n",
    "        q_action = tf.reduce_sum(tf.multiply(self.q_values, action_one_hot), axis=1)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.square(y_input - q_action))\n",
    "        optimizer = tf.train.AdamOptimizer(OPTIMIZER_LEARNING_RATE).minimize(loss)\n",
    "        \n",
    "        return action_input, y_input, loss, optimizer\n",
    "        \n",
    "    def run(self, state, action, reward, next_state, terminal):        \n",
    "        # Store transition in memory\n",
    "        self.memory_replay.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        if len(self.memory_replay) > MEMORY_REPLAY_SIZE:\n",
    "            self.memory_replay.popleft()\n",
    "            \n",
    "        if len(self.memory_replay) > BATCH_SIZE:\n",
    "            self.train_network()\n",
    "            \n",
    "    def get_action(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0, self.num_actions-1)\n",
    "        else:\n",
    "            return np.argmax(self.q_values.eval(feed_dict={self.state_input:[state]}))\n",
    "        \n",
    "        self.epsilon -= self.epsilon_step\n",
    "        \n",
    "    def train_network(self):\n",
    "        mini_batch = pd.DataFrame(random.sample(self.memory_replay, BATCH_SIZE), columns = ['state','action','reward','next_state','terminal'])\n",
    "        \n",
    "        q_values_batch = self.q_values.eval(feed_dict={self.state_input:mini_batch['next_state'].tolist()})\n",
    "    \n",
    "        terminal_batch = mini_batch['terminal'] + 0 # convert True to 1, False to 0\n",
    "        \n",
    "        y_batch = mini_batch['reward'] + (1 - terminal_batch) * GAMMA * np.max(q_values_batch, axis=1)\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss,self.optimizer], feed_dict={\n",
    "            self.y_input : y_batch.tolist(),\n",
    "            self.action_input : mini_batch['action'].tolist(),\n",
    "            self.state_input : mini_batch['state'].tolist()\n",
    "        })\n",
    "        \n",
    "        self.time_step += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode : ', 0, 'avg_reward:', 43.6)\n",
      "[array([ 0.05355016, -0.02047127, -0.00641891, -0.01344002]), array([ 0.10347478, -0.1868039 , -0.1805973 , -0.36140569]), array([ 0.11113688, -0.00283495, -0.14220991, -0.40632803]), array([ 0.11586891, -0.20669189, -0.11969814,  0.08356857]), array([ 0.06403475,  0.56525806, -0.02490905, -0.89962851]), array([ 0.04740886,  0.36942301,  0.00579298, -0.59113394]), array([ 0.11513017, -0.19966479, -0.14075475, -0.07275773]), array([ 0.0997387 ,  0.0103638 , -0.18782542, -0.70515476]), array([ 0.04391943,  0.17447175,  0.01183713, -0.30220757]), array([ 0.03340466, -0.01942256,  0.03100186, -0.03658879]), array([ 0.05828173, -0.02081504, -0.01196234, -0.00585669]), array([ 0.1152639 , -0.00668645, -0.13435002, -0.32023654]), array([ 0.03652104,  0.36991948,  0.02388346, -0.60231663]), array([ 0.11108018, -0.19568416, -0.15033647, -0.16164331]), array([ 0.11253642,  0.18003418, -0.10774949, -0.42714945]), array([ 0.05314074,  0.17474215, -0.00668771, -0.30814126]), array([ 0.11153356,  0.18651704, -0.12291383, -0.57180963]), array([ 0.08274956,  0.17598529, -0.05519918, -0.33601015]), array([ 0.10506172,  0.37373499, -0.09397286, -0.68883131]), array([ 0.08626927,  0.37184757, -0.06191939, -0.64557642]), array([ 0.10336265,  0.00560631, -0.16866585, -0.59657245]), array([ 0.01462738,  0.00706819, -0.02912911,  0.00200538]), array([ 0.11613711, -0.01340986, -0.11629248, -0.17028324]), array([ 0.09994598, -0.1817279 , -0.20192851, -0.47698137]), array([ 0.07533992,  0.37048237, -0.04290162, -0.61487811]), array([ 0.05786543, -0.21576341, -0.01207947,  0.2830281 ]), array([ 0.09370622,  0.56777521, -0.07483092, -0.95709734]), array([ 0.05479732,  0.17422043, -0.0060297 , -0.29663186]), array([ 0.10719118, -0.19142665, -0.16352378, -0.25710371]), array([ 0.03301621,  0.17524142,  0.03027009, -0.31933136]), array([ 0.1071665 ,  0.00123448, -0.15356933, -0.49772223]), array([ 0.11173507, -0.01007559, -0.11802677, -0.24435288])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (32,) for Tensor u'Placeholder_10:0', which has shape '(?, 2)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-73cd90fa85dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1f779924c03e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, state, action, reward, next_state, terminal)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_replay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1f779924c03e>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_input\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_input\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_input\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         })\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Steven/.virtualenvs/COMP-551/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Steven/.virtualenvs/COMP-551/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    944\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (32,) for Tensor u'Placeholder_10:0', which has shape '(?, 2)'"
     ]
    }
   ],
   "source": [
    "EPISODES = 10000\n",
    "env = gym.make(ENV_NAME)\n",
    "agent = Agent(env)\n",
    "\n",
    "for episode in xrange(EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.run(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "    if episode % 100 == 0:\n",
    "        total_reward = 0\n",
    "        for i in xrange(10): #10 test\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "#                 env.render()\n",
    "                action = agent.get_action(state)\n",
    "                state,reward,done,_ = env.step(action)\n",
    "                total_reward += reward\n",
    "        avg_reward = total_reward / 10\n",
    "        print('episode : ', episode, \"avg_reward:\", avg_reward)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
