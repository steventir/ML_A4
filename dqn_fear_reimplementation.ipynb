{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "ENV_NAME = \"CartPole-v0\"\n",
    "\n",
    "# FEAR MODEL\n",
    "fear_on = False\n",
    "fear_radius = 5\n",
    "fear_factor = 0.5\n",
    "fear_linear = 1000000\n",
    "fear_warmup = 20000\n",
    "\n",
    "# DQN\n",
    "GAMMA = 0.9 # Discount factor\n",
    "INITIAL_EPSILON = 0.5 # Initial value for epsilon-greedy\n",
    "FINAL_EPSILON = 0.01 # Final value for epsilon-greedy\n",
    "EXPLORATION_STEPS = 10000 # Number of steps till epsilon reaches its final value\n",
    "MEMORY_REPLAY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER_LEARNING_RATE = 0.0001\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        \"\"\" Initilizes the agent: replay memory, neural network \"\"\"\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.num_observations = env.observation_space.shape[0]\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "        self.time_step = 0\n",
    "        \n",
    "        # Create memory buffer for replay\n",
    "        self.memory_replay = deque()\n",
    "        \n",
    "        # Create Q network and define loss\n",
    "        self.state_input, self.q_values = self.build_network()\n",
    "        self.action_input, self.y_input, self.loss, self.optimizer = self.build_training_ops()  \n",
    "        \n",
    "        # Create session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_network(self):\n",
    "        \"\"\" Constructs the neural network with 1 hidden layer to approximate the Q function \"\"\"\n",
    "        W1 = tf.Variable(tf.truncated_normal([self.num_observations,16]))\n",
    "        b1 = tf.Variable(tf.constant(0.01, shape=[16]))\n",
    "        W2 = tf.Variable(tf.truncated_normal([16,self.num_actions]))\n",
    "        b2 = tf.Variable(tf.constant(0.01, shape=[self.num_actions]))\n",
    "\n",
    "        state_input = tf.placeholder(dtype=tf.float32)\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(state_input,W1) + b1)\n",
    "        q_values = tf.matmul(hidden_layer, W2) + b2\n",
    "\n",
    "        return state_input, q_values\n",
    "        \n",
    "    def build_training_ops(self):\n",
    "        \"\"\" Defines the loss function \"\"\"\n",
    "        y_input = tf.placeholder(dtype=tf.float32)\n",
    "        action_input = tf.placeholder(dtype=tf.int32)\n",
    "        action_one_hot = tf.one_hot(action_input, self.num_actions, on_value=1.0, off_value=0.0)\n",
    "\n",
    "        q_action = tf.reduce_sum(tf.multiply(self.q_values, action_one_hot), axis=1)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.square(y_input - q_action))\n",
    "        optimizer = tf.train.AdamOptimizer(OPTIMIZER_LEARNING_RATE).minimize(loss)\n",
    "        \n",
    "        return action_input, y_input, loss, optimizer\n",
    "        \n",
    "    def run(self, state, action, reward, next_state, terminal):   \n",
    "        \"\"\"  \"\"\"\n",
    "        # Store transition in memory\n",
    "        self.memory_replay.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        if len(self.memory_replay) > MEMORY_REPLAY_SIZE:\n",
    "            self.memory_replay.popleft()\n",
    "            \n",
    "        if len(self.memory_replay) > BATCH_SIZE:\n",
    "            self.train_network()\n",
    "            \n",
    "    def explore(self, state):\n",
    "        \"\"\" Explores possible actions using epsilon-greedy with epsilon linearly decreasing \"\"\"\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0, self.num_actions-1)\n",
    "        else:\n",
    "            return np.argmax(self.q_values.eval(feed_dict={self.state_input:[state]}))\n",
    "        \n",
    "        self.epsilon -= self.epsilon_step\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\" Get the best action : max Q \"\"\"\n",
    "        return np.argmax(self.q_values.eval(feed_dict={self.state_input:[state]}))\n",
    "        \n",
    "    def train_network(self):\n",
    "        \"\"\" Trains the network using mini batch \"\"\"\n",
    "        mini_batch = pd.DataFrame(random.sample(self.memory_replay, BATCH_SIZE), columns = ['state','action','reward','next_state','terminal'])\n",
    "        \n",
    "        q_values_batch = self.q_values.eval(feed_dict={self.state_input:mini_batch['next_state'].tolist()})\n",
    "        terminal_batch = mini_batch['terminal'] + 0 # convert True to 1, False to 0\n",
    "        \n",
    "        y_batch = mini_batch['reward'] + (1 - terminal_batch) * GAMMA * np.max(q_values_batch, axis=1)\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss,self.optimizer], feed_dict={\n",
    "            self.y_input : y_batch.tolist(),\n",
    "            self.action_input : mini_batch['action'].tolist(),\n",
    "            self.state_input : mini_batch['state'].tolist()\n",
    "        })\n",
    "        \n",
    "        self.time_step += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode : ', 0, 'avg_reward:', 9.7)\n",
      "('episode : ', 100, 'avg_reward:', 27.7)\n",
      "('episode : ', 200, 'avg_reward:', 29.8)\n",
      "('episode : ', 300, 'avg_reward:', 11.6)\n",
      "('episode : ', 400, 'avg_reward:', 9.2)\n",
      "('episode : ', 500, 'avg_reward:', 9.4)\n",
      "('episode : ', 600, 'avg_reward:', 14.8)\n",
      "('episode : ', 700, 'avg_reward:', 40.9)\n",
      "('episode : ', 800, 'avg_reward:', 57.2)\n",
      "('episode : ', 900, 'avg_reward:', 138.3)\n",
      "('episode : ', 1000, 'avg_reward:', 124.5)\n",
      "('episode : ', 1100, 'avg_reward:', 134.4)\n",
      "('episode : ', 1200, 'avg_reward:', 153.0)\n",
      "('episode : ', 1300, 'avg_reward:', 200.0)\n",
      "('episode : ', 1400, 'avg_reward:', 200.0)\n",
      "('episode : ', 1500, 'avg_reward:', 150.8)\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 10000\n",
    "env = gym.make(ENV_NAME)\n",
    "agent = Agent(env)\n",
    "\n",
    "for episode in xrange(EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.run(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "    if episode % 100 == 0:\n",
    "        total_reward = 0\n",
    "        for i in xrange(10): #10 test\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "#                 env.render()\n",
    "                action = agent.get_action(state)\n",
    "                state,reward,done,_ = env.step(action)\n",
    "                total_reward += reward\n",
    "        avg_reward = total_reward / 10\n",
    "        print('episode : ', episode, \"avg_reward:\", avg_reward)\n",
    "        if avg_reward >= 200:\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
